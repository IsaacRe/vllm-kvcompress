# -*- coding: utf-8 -*-
"""KV Compress - multi-head eviction validation - final mod.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GVkTtWIC4Ua6sdtPTQEGm_3UQgevLUt1
"""

# !pip install transformers==4.35.2 datasets accelerate bitsandbytes sentencepiece safetensors py7zr

# Commented out IPython magic to ensure Python compatibility.
import os
import numpy as np
import torch
import torch.nn.functional as F
from tqdm.auto import tqdm
import matplotlib.pyplot as plt
from argparse import ArgumentParser

parser = ArgumentParser()
parser.add_argument('--n-sample', type=int, default=50)
parser.add_argument('--ctx-len', type=int, default=2048)
parser.add_argument('--out-dir', type=str, default='kv-bias')
parser.add_argument('--quantization', type=str, choices=['4bit', '8bit', None], default=None)
parser.add_argument('--model', type=str, choices=['mistral', 'llama3'])
parser.add_argument('--test-set', action='store_true')
args = parser.parse_args()

from kvcompress.config import CONFIG
from kvcompress.globals import GLOBALS
from kvcompress.modify_llama import get_llama
from kvcompress.data import make_random_digit_data
from kvcompress.plot_util import plot_iterative
from kvcompress.run_util import run_model
from kvcompress.compress import quantize_iterative_sporadicity

from transformers import LlamaForCausalLM, AutoTokenizer
from kvcompress.modify_llama import modify_llama


ANCHOR_OUTPUT_LEN = 512

def get_llama(model_name):
    load_in_8bit = args.quantization == '8bit'
    load_in_4bit = args.quantization == '4bit'
    model = LlamaForCausalLM.from_pretrained(model_name,
                                         #quantization_config=q_config,
                                         load_in_8bit=load_in_8bit,
                                         load_in_4bit=load_in_4bit,
                                         device_map='cuda:0',
                                             revision=CONFIG.revision,
    )
    tokenizer = AutoTokenizer.from_pretrained(model_name,
                                              revision=CONFIG.revision)
    modify_llama(model)
    return model, tokenizer

if args.model == 'mistral':
    CONFIG.revision = 'b70aa86578567ba3301b21c8a27bea4e8f6d6d61'
    CONFIG.model_name = "mistralai/Mistral-7B-Instruct-v0.2"
elif args.model == 'llama3':
    CONFIG.revision = '5206a32e0bd3067aef1ce90f5528ade7d866253f'
    CONFIG.model_name = "meta-llama/Meta-Llama-3.1-8B-Instruct"
else:
    raise ValueError("invalid model")
model, tokenizer = get_llama(CONFIG.model_name)

"""Profile Sporadicity"""

def compress_iterative_sporadicity(sp, log=2):
    merged_sp = torch.stack([sp[l] for l in range(32)])
    out_sp = []
    last_i = 0
    i = log
    k = 0
    np.log
    bins = [last_i]
    while i < merged_sp.shape[-1]:
        if k > 10000:
            break
        k += 1
        out_sp.append(merged_sp[:,0,:,last_i:i].mean(dim=-1))
        last_i = i
        bins.append(last_i)
        i = i ** 2
    out_sp.append(merged_sp[:,0,:,last_i:-1].mean(dim=-1))
    return torch.stack(out_sp, dim=-1), torch.tensor(bins)

def expand_compressed_sporadicity(bias, bins, max_len=1600):
    sp = torch.cat([
        bias[...,j:j+1].repeat(
            1,1,(bins[j+1] if j+1 < bins.size(0) else max_len)-bins[j]
        )
        for j in range(bins.size(0))
    ], dim=-1)
    return {i: sp[i,None] for i in range(sp.size(0))}

def compute_iterative_sporadicity_snapkv(rescaled_attn):
    window_size = getattr(CONFIG, 'window_size', 32)

    start_response = CONFIG.consolidate_range[1]
    start_observed = start_response - window_size
    seen, unseen = (  # [batch, heads, 1, k_len]
        (rescaled_attn[...,start_observed:start_response,:start_observed] ** 2).sum(dim=-2),
        (rescaled_attn[...,start_response:,:start_observed] ** 2).sum(dim=-2),
    )
    return (unseen - seen).mean(dim=0).mean(dim=-1)[None,...,None]  # [1, heads, 1, 1]

def compute_iterative_sporadicity_snapkv_v2(rescaled_attn):
    window_size = getattr(CONFIG, 'window_size', 32)

    start_response = CONFIG.consolidate_range[1]
    start_observed = start_response - window_size
    seen, unseen = (  # [batch, heads, 1, k_len]
        (rescaled_attn[...,start_observed:start_response,:start_observed] ** 2).sum(dim=-2),
        (rescaled_attn[...,start_response:,:start_observed] ** 2).sum(dim=-2),
    )
    mask = unseen > seen
    p = mask.type(torch.float).mean(dim=0).mean(dim=-1) ** 2
    return (
        torch.clamp(unseen - seen, min=0).mean(dim=0).mean(dim=-1)[None,...,None]  # [1, heads, 1, 1]
        * p[None,...,None]
    )

import kvcompress.modify_llama
kvcompress.modify_llama.compute_iterative_sporadicity_v2 = compute_iterative_sporadicity_snapkv_v2

import json
dataset2prompt = json.load(open("config/dataset2prompt.json", "r"))

subset2dataset = {
    'narrativeqa': ('deepmind/narrativeqa', None),  # dataset, subset
    'qasper': ('allenai/qasper', None),
    'hotpotqa': ('hotpotqa/hotpot_qa', 'fullwiki'),
    '2wikimqa': ('xanhho/2WikiMultihopQA', None),
    'musique': ('bdsaglam/musique', 'full'),
    'gov_report': ('ccdv/govreport-summarization', None),
    'qmsum': ('pszemraj/qmsum-cleaned', 'no-prefix'),
    'multi_news': ('alexfabbri/multi_news', None),
    'trec': ('CogComp/trec', None),
    'triviaqa': ('mandarjoshi/trivia_qa', None),
    'samsum': ('Samsung/samsum', None),
    'lcc': ('microsoft/LCC_python', None),
    'repobench-p': ('tianyang/repobench-p',
                    {'name': 'python', 'split': 'cff'}),
}

def get_qasper_document(r):
    section_text = r['full_text']['paragraphs']
    section_name = r['full_text']['section_name']
    out = f"{r['title']}\n\nAbstract\n{r['abstract']}\n\n"
    out += '\n\n'.join([f"{s}\n{t}" for s, t in zip(section_name,
                                                    section_text)])
    return out

def get_qasper_question_answer(r, random=False):
    idx = 0
    r = r['qas']
    if random:
        idx = np.random.randint(len(r['question']))
    question = r['question'][idx]
    answer = r['answers'][idx]['answer'][0]
    if answer['unanswerable']:
        return question, 'unanswerable'
    if answer['yes_no'] is not None:
        return question, answer['yes_no']
    return question, answer['free_form_answer']

def get_lcc_document_question_answer(r):
    lines = r['context'].split('\n')
    line_no = np.random.randint(len(lines) // 2, len(lines))
    return '\n'.join(lines[:line_no]), None, lines[line_no]


format_record = {
    'narrativeqa': lambda r: (r['document']['text'],     # context
                              r['question']['text'],     # input
                              r['answers'][0]['text']),  # output
    'qasper': lambda r: (get_qasper_document(r),
                         *get_qasper_question_answer(r, random=True)),
    'hotpotqa': lambda r: ('\n\n'.join([t + '\n' + ' '.join(s)
                                        for t, s
                                        in zip(r['context']['title'],
                                               r['context']['sentences'])]),
                           r['question'],
                           r['answer']),
    '2wikimqa': lambda r: ('\n\n'.join([t + '\n' + ' '.join(s)
                                        for t, s
                                        in zip(r['context']['title'],
                                               r['context']['content'])]),
                           r['question'],
                           r['answer']),
    'musique': lambda r: ('\n\n'.join([t + '\n' + s
                                        for t, s
                                        in zip(r['paragraphs']['title'],
                                               r['paragraphs']['paragraph_text'])]),
                           r['question'],
                           r['answer'] if r['answerable'] else 'unanswerable'),
    'gov_report': lambda r: (r['report'],
                             None,
                             r['summary']),
    'qmsum': lambda r: (r['input'], r['prompt'], r['output']),
    'multi_news': lambda r: (r['document'], None, r['summary']),
    'lcc': lambda r: get_lcc_document_question_answer(r),
}

from datasets import load_dataset
from kvcompress.run_util import compute_loss

def get_longbench(tokenizer, n_samples: int):
    dset, subset = subset2dataset[CONFIG.subset]
    data = load_dataset(dset,
                        subset,
                        split='train',
                        streaming=True,
                        trust_remote_code=True)
    # data = load_dataset(
    #     'deepmind/narrativeqa', split='train', streaming=True
    # )

    inputs = []
    max_length = CONFIG.consolidate_range[1]
    prompt_format = dataset2prompt[CONFIG.subset]
    for record in tqdm(data.take(n_samples), total=n_samples):
        context_text, question_text, output_text = format_record[CONFIG.subset](record)
        # context_text = record['document']['text']
        # question_text = record['question']['text']
        # output_text = f"\n\n{record['answers'][0]['text']}"
        # input_text = NARRATIVE_QA_PROMPT.format(context=context_text, input=question_text)
        output_text = f"\n\n{output_text}"
        input_text = prompt_format.format(context=context_text, input=question_text)
        tokenized_inp = tokenizer.batch_encode_plus([input_text], return_tensors="pt", truncation=False)
        tokenized_out = tokenizer.batch_encode_plus([output_text], return_tensors="pt", truncation=False, add_special_tokens=False)
        new_inputs = {}
        for name, t in tokenized_inp.items():
            shortened_inp = torch.cat([t[:,:max_length//2], t[:,-max_length//2-1:]], dim=-1)
            out = tokenized_out[name]
            tensor_inp = torch.cat([shortened_inp, out], dim=1).to(0)
            new_inputs[name] = tensor_inp
        inputs.append(new_inputs)
    return inputs


def get_longbench_test(tokenizer, n_samples: int):
    data = load_dataset("THUDM/LongBench",
                        CONFIG.subset,
                        split='test',
                        streaming=True,
                        trust_remote_code=True)
    print("Loading LongBench test sets")
    data = list(data)
    selected_indices = np.random.choice(len(data), n_samples, replace=False)
    np.save(os.path.join(args.out_dir, f"selected_indices-{CONFIG.subset}.npy"), selected_indices)
    data = [data[i] for i in selected_indices]
    inputs = []
    max_length = CONFIG.consolidate_range[1]
    prompt_format = dataset2prompt[CONFIG.subset]
    for record in tqdm(data):
        input_text = prompt_format.format(**record)
        output_text = f"\n\n{record['answers'][0]}"
        tokenized_inp = tokenizer.batch_encode_plus([input_text], return_tensors="pt", truncation=False)
        tokenized_out = tokenizer.batch_encode_plus([output_text], return_tensors="pt", truncation=False, add_special_tokens=False)
        new_inputs = {}
        for name, t in tokenized_inp.items():
            shortened_inp = torch.cat([t[:,:max_length//2], t[:,-max_length//2-1:]], dim=-1)
            out = tokenized_out[name]
            tensor_inp = torch.cat([shortened_inp, out], dim=1).to(0)
            new_inputs[name] = tensor_inp
        inputs.append(new_inputs)
    return inputs


def run_model(model, tokenizer, n_samples: int = 1, model_inputs = None):
    GLOBALS.reset()
    CONFIG.use_cache = False
    if model_inputs is None:
        if CONFIG.dataset == 'pg19':
            model_inputs = get_pg19(tokenizer, n_samples)
        elif CONFIG.dataset == 'longbench':
            model_inputs = get_longbench(tokenizer, n_samples)
        elif CONFIG.dataset == 'longbench-test':
            model_inputs = get_longbench_test(tokenizer, n_samples)
        else:
            model_inputs = [make_random_digit_data(tokenizer) for _ in range(n_samples)]
    output_lens = []
    with torch.no_grad():
        for model_input in tqdm(model_inputs):
            compute_loss(model, model_input)  # forward pass to collect attention weights
            output_lens.append(model_input['input_ids'].size(1) - CONFIG.consolidate_range[1])
    iterative_sporadicity = {
        l: sum(GLOBALS.vars.get(f'layer{l}_iterative_sporadicity', [None])) / n_samples
        for l in range(CONFIG.num_layers)
    }
    relative_interative_sporadicity = {
        l: sum([t * ANCHOR_OUTPUT_LEN / out_len
                for t, out_len in
                zip(GLOBALS.vars.get(f'layer{l}_iterative_sporadicity', [None]), output_lens)]) / n_samples
        for l in range(CONFIG.num_layers)
    }
    return iterative_sporadicity, relative_interative_sporadicity

CONFIG.context_length = args.ctx_len
CONFIG.consolidate_range = 0, args.ctx_len
CONFIG.record_layer_idx = []
CONFIG.do_consolidate = False
CONFIG.profile_iterative_sporadicity = 'v2'
CONFIG.dataset = 'longbench-test' if args.test_set else 'longbench'

os.makedirs(args.out_dir, exist_ok=True)
for subset in format_record:
    CONFIG.subset = subset
    subset_sporadicity, relative_subset_sporadicity = run_model(model, tokenizer, args.n_sample)

    bias = torch.stack([subset_sporadicity[i] for i in range(32)], dim=0)[:,0]
    # aggregate sporadicity for each GQA head
    q_heads_per_kv_head = model.config.num_attention_heads // model.config.num_key_value_heads
    bias = torch.stack([t.mean(dim=1) for t in bias.split(q_heads_per_kv_head, dim=1)], dim=1)
    np.savez(f"{args.out_dir}/kv_head_bias_{args.model}-{subset}.npz", bias=bias.cpu().numpy(), pos_bins=np.array([0]))

    bias = torch.stack([relative_subset_sporadicity[i] for i in range(32)], dim=0)[:,0]
    # aggregate sporadicity for each GQA head
    q_heads_per_kv_head = model.config.num_attention_heads // model.config.num_key_value_heads
    bias = torch.stack([t.mean(dim=1) for t in bias.split(q_heads_per_kv_head, dim=1)], dim=1)
    np.savez(f"{args.out_dir}/kv_head_bias_{args.model}-{subset}-relative.npz",
             bias=bias.cpu().numpy(), pos_bins=np.array([0]), anchor=np.array(ANCHOR_OUTPUT_LEN))
